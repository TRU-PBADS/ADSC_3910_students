{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Writing unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Unit testing is a fundamental practice in software development, including data science projects. It ensures that individual components of your code work as expected. This lecture will cover the purpose of unit testing, the basic structure of a unit test, the differences between `unittest` and `pytest`, and examples of writing unit tests for various data types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Purpose of Unit Testing in Data Science Projects\n",
    "\n",
    "### Analogy: Building a House\n",
    "\n",
    "Imagine it's December in Kamloops, and the weather is getting cold. You notice that the heating system in your house is not working properly. To fix the issue, you need to identify the root cause. Is it a problem with the gas supply? Is there an issue with the electrical wiring? Or is the insulation layer of the house too thin, causing heat to escape?\n",
    "\n",
    "Just like diagnosing the heating system in your house, unit testing in a data science project helps you pinpoint specific issues in your code. By testing individual components, you can quickly identify and fix problems, ensuring that the overall system functions correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Key Benefits:\n",
    "- **Ensures Code Quality:** Catches bugs early in the development process.\n",
    "- **Facilitates Refactoring:** Allows you to refactor code confidently, knowing that existing functionality is preserved.\n",
    "- **Improves Collaboration:** Provides a clear understanding of code functionality for team members.\n",
    "- **Documentation:** Acts as documentation for the code, explaining what each function is supposed to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Basic Structure of a Unit Test\n",
    "\n",
    "### Components of a Unit Test:\n",
    "Most functional tests follow the Arrange-Act-Assert model:\n",
    "\n",
    "1. **Arrange**, or set up, the conditions for the test\n",
    "2. **Act** by calling some function or method\n",
    "3. **Assert** that some end condition is true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Example \n",
    "Let's write a simple function to calculate the mean of a list of numbers and write a unit test for it.\n",
    "\n",
    "**Function to Test:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(list_numbers):\n",
    "    return sum(list_numbers) / len(list_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mean([1, 2, 3, 4, 5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Unit Test:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange\n",
    "list_numbers = [1,2,3,4,5]\n",
    "\n",
    "# act\n",
    "output = calculate_mean(list_numbers)\n",
    "\n",
    "# assert\n",
    "assert output == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap it inside a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_mean():\n",
    "    # arrange\n",
    "    list_numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "    # act\n",
    "    output = calculate_mean(list_numbers)\n",
    "\n",
    "    # assert\n",
    "    assert output == 3, f\"Expected 3, but got {output}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: Writing a meaningful error message \n",
    "\n",
    "You can use f string to write a meaningful error message if the test case fail so that it's easier for future you and other people to understand why the program is failing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to temper with our original `calculate_mean` function by adding 3 to the numerator and see if the unit test we wrote will manage to catch the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(list_numbers):\n",
    "    return sum(list_numbers) + 3/ len(list_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected 3, but got 15.6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_calculate_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 9\u001b[0m, in \u001b[0;36mtest_calculate_mean\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m calculate_mean(list_numbers)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# assert\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m output \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 3, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected 3, but got 15.6"
     ]
    }
   ],
   "source": [
    "test_calculate_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah ha! You could see that our unit test has been able to catch this mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put everything in Python scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a script called `mean.py` that contains the `calculate_mean()` function\n",
    "\n",
    "```python\n",
    "# mean.py\n",
    "def calculate_mean(list_numbers):\n",
    "    return sum(list_numbers) / len(list_numbers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put the unit tests in a Python script called `test_mean.py` and execute it again. First, you need to import the `calculate_mean` function from the `mean.py` module\n",
    "\n",
    "```python\n",
    "from mean import calculate_mean\n",
    "\n",
    "def test_calculate_mean():\n",
    "    # arrange\n",
    "    list_numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "    # act\n",
    "    output = calculate_mean(list_numbers)\n",
    "\n",
    "    # assert\n",
    "    assert output == 3, f\"Expected 3, but got {output}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_calculate_mean()\n",
    "    print(\"Everything passed!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's open a terminal and run `python test_mean.py` to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything passed!\n"
     ]
    }
   ],
   "source": [
    "!python test_mean.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choosing a test runner\n",
    "\n",
    "There are two main test runners: `unittest` which is a built-in python module, and `pytest`. I usually prefer to use `pytest` since the syntax is more simple and consise, and it also provides more detailed error message and tracebacks. \n",
    "\n",
    "Let's have a look at both and see their differences\n",
    "\n",
    "\n",
    "### `unittest`:\n",
    "\n",
    "`unittest` requires that:\n",
    "\n",
    "- You put your tests into **classes** as methods\n",
    "- You use a series of **special assertion methods** in the unittest.TestCase class instead of the built-in assert statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_mean_unittest.py\n",
    "\n",
    "import unittest\n",
    "from mean import calculate_mean\n",
    "\n",
    "class TestCalculateMean(unittest.TestCase):\n",
    "    def test_calculate_mean(self):\n",
    "        # arrange\n",
    "        list_numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "        # act\n",
    "        output = calculate_mean(list_numbers)\n",
    "\n",
    "        # assert\n",
    "        self.assertEqual(output, 3, f\"Expected 3, but got {output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_calculate_mean (__main__.TestCalculateMean.test_calculate_mean)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/quannguyen/TRU-PBADS/ADSC_3910_instructors/lectures/test_mean_unittest.py\", line 13, in test_calculate_mean\n",
      "    self.assertEqual(output, 3, f\"Expected 3, but got {output}\")\n",
      "AssertionError: 17.0 != 3 : Expected 3, but got 17.0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "!python test_mean_unittest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Pros:**\n",
    "  - Built-in Python module.\n",
    "  - Provides a comprehensive framework for writing and running tests.\n",
    "  - Supports test discovery and fixtures.\n",
    "- **Cons:**\n",
    "  - Verbose syntax.\n",
    "  - Limited to the features provided by the standard library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### `pytest`:\n",
    "\n",
    "`pytest` supports execution of `unittest` test cases. The real advantage of pytest comes by writing pytest test cases. pytest test cases are a series of functions in a Python file starting with the name test_.\n",
    "\n",
    "`pytest` has some other great features:\n",
    "\n",
    "- Support for the built-in assert statement instead of using special self.assert*() methods\n",
    "- Support for filtering for test cases\n",
    "- Ability to rerun from the last failing test\n",
    "- An ecosystem of hundreds of plugins to extend the functionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.5, pytest-8.3.2, pluggy-1.5.0\n",
      "rootdir: /Users/quannguyen/TRU-PBADS/ADSC_3910_instructors/lectures\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_mean.py \u001b[31mF\u001b[0m\u001b[31m                                                           [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____________________________ test_calculate_mean ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_calculate_mean\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# arrange\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        list_numbers = [\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# act\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        output = calculate_mean(list_numbers)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# assert\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m output == \u001b[94m3\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mExpected 3, but got \u001b[39;49;00m\u001b[33m{\u001b[39;49;00moutput\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: Expected 3, but got 17.0\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 17.0 == 3\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_mean.py\u001b[0m:11: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_mean.py::\u001b[1mtest_calculate_mean\u001b[0m - AssertionError: Expected 3, but got 17.0\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.05s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_mean.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pros:**\n",
    "  - Simple and concise syntax.\n",
    "  - Supports fixtures, parameterized tests, and plugins.\n",
    "  - Provides detailed error messages and tracebacks.\n",
    "  - Can run `unittest` tests.\n",
    "- **Cons:**\n",
    "  - Requires installation of an external package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Why `pytest` is Preferred:\n",
    "- **Ease of Use:** `pytest` has a more straightforward and readable syntax.\n",
    "- **Flexibility:** Supports advanced features like fixtures and parameterized tests.\n",
    "- **Extensibility:** Offers a wide range of plugins to extend its functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to write the tests\n",
    "\n",
    "Typically this is how a project directory structure looks like:\n",
    "\n",
    "```python\n",
    "project/\n",
    "│\n",
    "├── src/\n",
    "│   └── function_1.py\n",
    "│   └── function_2.py\n",
    "├── tests/\n",
    "│   └── test_function_1.py\n",
    "│   └── test_function_2.py\n",
    "```\n",
    "\n",
    "You’ll find that, as you add more and more tests, your single file will become cluttered and hard to maintain, so you can create a folder called `tests/` and split the tests into multiple files. It is convention to ensure each file starts with test_ so all test runners will assume that Python file contains tests to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing `assert` statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Assert Statement          | Example                                                                                  |\n",
    "|---------------------------|------------------------------------------------------------------------------------------|\n",
    "| `assert a == b`           | `assert calculate_mean([1, 2, 3]) == 2`                                                  |\n",
    "| `assert a != b`           | `assert calculate_mean([1, 2, 3]) != 3`                                                  |\n",
    "| `assert x is True`        | `assert isinstance(calculate_mean([1, 2, 3]), float) is True`                            |\n",
    "| `assert x is False`       | `assert (calculate_mean([1, 2, 3]) == 0) is False`                                       |\n",
    "| `assert a is b`           | `assert calculate_mean([1, 2, 3]) is calculate_mean([1, 2, 3])`                          |\n",
    "| `assert a is not b`       | `assert calculate_mean([1, 2, 3]) is not calculate_mean([4, 5, 6])`                      |\n",
    "| `assert x is None`        | `assert None is None`                                                                    |\n",
    "| `assert x is not None`    | `assert calculate_mean([1, 2, 3]) is not None`                                           |\n",
    "| `assert a in b`           | `assert 3 in [1, 2, 3, 4, 5]`                                                            |\n",
    "| `assert a not in b`       | `assert 6 not in [1, 2, 3, 4, 5]`                                                        |\n",
    "| `assert isinstance(a, b)` | `assert isinstance(calculate_mean([1, 2, 3]), float)`                                     |\n",
    "| `assert not isinstance(a, b)`| `assert not isinstance(calculate_mean([1, 2, 3]), int)`                                |\n",
    "| `assert a == pytest.approx(b)` | `assert calculate_mean([1, 2, 3]) == pytest.approx(2.0, rel=1e-2)`                   |\n",
    "| `assert a > b`            | `assert calculate_mean([4, 5, 6]) > 3`                                                   |\n",
    "| `assert a >= b`           | `assert calculate_mean([3, 4, 5]) >= 3`                                                  |\n",
    "| `assert a < b`            | `assert calculate_mean([1, 2, 3]) < 4`                                                   |\n",
    "| `assert a <= b`           | `assert calculate_mean([1, 2, 3]) <= 3`                                                  |\n",
    "| `assert re.search(r, s)`  | `assert re.search(r\"hello\", \"hello world\")`                                              |\n",
    "| `assert not re.search(r, s)`| `assert not re.search(r\"bye\", \"hello world\")`                                           |\n",
    "| `assert sorted(a) == sorted(b)` | `assert sorted([1, 2, 3]) == sorted([3, 2, 1])`                                     |\n",
    "\n",
    "Note: `pytest.approx` is used for approximate comparisons, useful for floating-point comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing numeric output\n",
    "\n",
    "When working with floats, set a precision level that is appropriate for your application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m      2\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m a \u001b[38;5;241m==\u001b[39m b\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = 0.1 + 0.2\n",
    "b = 0.3\n",
    "\n",
    "assert a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000000000000004\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "assert a == pytest.approx(b, rel=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m list_a \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbanana\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapple\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkiwi\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m list_b \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapple\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbanana\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkiwi\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m list_a \u001b[38;5;241m==\u001b[39m list_b\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_a = [\"banana\", \"apple\", \"kiwi\"]\n",
    "list_b = [\"apple\", \"banana\", \"kiwi\"]\n",
    "\n",
    "assert list_a == list_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort both lists before comparing\n",
    "assert sorted(list_a) == sorted(list_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "df_a = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df_b = pd.DataFrame({'B': [3, 4], 'A': [1, 2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "0  1  3\n",
       "1  2  4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   B  A\n",
       "0  3  1\n",
       "1  4  2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "DataFrame.columns are different\n\nDataFrame.columns values are different (100.0 %)\n[left]:  Index(['A', 'B'], dtype='object')\n[right]: Index(['B', 'A'], dtype='object')\nAt positional index 0, first diff: A != B",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43massert_frame_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_b\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32mtesting.pyx:55\u001b[0m, in \u001b[0;36mpandas._libs.testing.assert_almost_equal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtesting.pyx:173\u001b[0m, in \u001b[0;36mpandas._libs.testing.assert_almost_equal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/adsc_3610/lib/python3.12/site-packages/pandas/_testing/asserters.py:614\u001b[0m, in \u001b[0;36mraise_assert_detail\u001b[0;34m(obj, message, left, right, diff, first_diff, index_values)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_diff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    612\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfirst_diff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: DataFrame.columns are different\n\nDataFrame.columns values are different (100.0 %)\n[left]:  Index(['A', 'B'], dtype='object')\n[right]: Index(['B', 'A'], dtype='object')\nAt positional index 0, first diff: A != B"
     ]
    }
   ],
   "source": [
    "assert_frame_equal(df_a, df_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ignore the columns' order when comparing dataframes, we can use the `check_like` option.\n",
    "When `check_like=True`, the function will sort the columns and rows before performing the comparison, ensuring that the DataFrames are considered equal if they contain the same data, regardless of the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_frame_equal(df_a, df_b, check_like=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexes also matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "# Example DataFrames with different indexes\n",
    "df_a = pd.DataFrame({\n",
    "    'A': [1, 2],\n",
    "    'B': [4, 5]\n",
    "}, index=[0, 1])\n",
    "\n",
    "df_b = pd.DataFrame({\n",
    "    'A': [1, 2],\n",
    "    'B': [4, 5]\n",
    "}, index=[2, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "0  1  4\n",
       "1  2  5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "2  1  4\n",
       "1  2  5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "DataFrame.index are different\n\nDataFrame.index values are different (50.0 %)\n[left]:  Index([0, 1], dtype='int64')\n[right]: Index([2, 1], dtype='int64')\nAt positional index 0, first diff: 0 != 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43massert_frame_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_b\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32mtesting.pyx:55\u001b[0m, in \u001b[0;36mpandas._libs.testing.assert_almost_equal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtesting.pyx:173\u001b[0m, in \u001b[0;36mpandas._libs.testing.assert_almost_equal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/adsc_3610/lib/python3.12/site-packages/pandas/_testing/asserters.py:614\u001b[0m, in \u001b[0;36mraise_assert_detail\u001b[0;34m(obj, message, left, right, diff, first_diff, index_values)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_diff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    612\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfirst_diff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: DataFrame.index are different\n\nDataFrame.index values are different (50.0 %)\n[left]:  Index([0, 1], dtype='int64')\n[right]: Index([2, 1], dtype='int64')\nAt positional index 0, first diff: 0 != 2"
     ]
    }
   ],
   "source": [
    "assert_frame_equal(df_a, df_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "0  1  4\n",
       "1  2  5"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert DataFrames are equal ignoring index differences\n",
    "assert_frame_equal(df_a.reset_index(drop=True), df_b.reset_index(drop=True), check_like=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_a = \"hello\"\n",
    "string_b = \"Hello\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "hello is not equal to Hello",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m string_a \u001b[38;5;241m==\u001b[39m string_b, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstring_a\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not equal to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstring_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: hello is not equal to Hello"
     ]
    }
   ],
   "source": [
    "assert string_a == string_b, f\"{string_a} is not equal to {string_b}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert string_a.lower() == string_b.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "Unit testing is essential for ensuring code quality and reliability in data science projects. By using frameworks like `unittest` and `pytest`, you can write tests that verify the correctness of your code. While `unittest` is built into Python, `pytest` is often preferred for its simplicity and flexibility.\n",
    "\n",
    "#### Key Takeaways:\n",
    "- Unit testing ensures that individual components of your code work as expected.\n",
    "- `unittest` and `pytest` are popular frameworks for writing unit tests in Python.\n",
    "- `pytest` is preferred for its ease of use and advanced features.\n",
    "- Write unit tests for various data types to ensure comprehensive test coverage.\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "- [Getting Started With Testing in Python](https://realpython.com/python-testing/)\n",
    "- [Python `unittest` Documentation](https://docs.python.org/3/library/unittest.html)\n",
    "- [pytest Documentation](https://docs.pytest.org/en/stable/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsc_3610",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
